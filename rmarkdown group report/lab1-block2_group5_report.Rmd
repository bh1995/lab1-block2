---
title: "lab1 block 2 group 5 report"
author: "Ahmed Alhasan,  Erik Anders, Bjorn_Hansen"
date: "12/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. ENSEMBLE METHODS

To start, the data is loaded into test and train sets with 2/3 of the data being used for training and 1/3 used for testing.  
```{r, echo=FALSE}
RNGversion('3.5.1')
library(readxl)
setwd("/home/erik/Documents/SML/Semester 1/Machine Learning/Lab 1 Block 2")
sp = read.csv2("spambase.csv")
sp$Spam = as.factor(sp$Spam)

n=dim(sp)[1]
set.seed(12345)
id=sample(1:n, floor(n*(2/3)))
train=sp[id,]
test=sp[-id,]
```

## Adaboost

Adaboost classification trees are used to train the model.Inside the boost control argument a value of 0.6 for "nu" was used as this value gave the lowest error. Adaboost predicts a loss score so to make a hard decision a threshold of 0.1 was used as this value gave roughly the lowest error. As can be seen from the confusion matrix below (using 100 trees) the diagonal values are quite high meaning that the ada model is predicting with high accuaracy. 

```{r, echo=FALSE}
library(mboost)
  ada_model = blackboost(Spam ~., data = train, control = boost_control(mstop = 100, nu = 0.6),
                       family = AdaExp())
  fitted.results_test= predict(ada_model,newdata=test)
  fitted.results_test = ifelse(fitted.results_test > 0.1,1,0)
  ada_confmat_test = table("Y"=test$Spam,"Y hat"=fitted.results_test)
  print(table("Y"=test$Spam,"Y hat"=fitted.results_test))
  
  print(paste('Error for ada model:',1-sum(diag(ada_confmat_test)) / sum(ada_confmat_test)))
```

 Below is a plot of the recorded error for 10 up to 100 trees. 
 
```{r, echo=FALSE}
library(mboost)
library(ggplot2)
acc=0
n = seq(10, 100, by = 10)
for(i in n){
  ada_model = blackboost(Spam ~., data = train, control = boost_control(mstop = i, nu = 0.6),
                       family = AdaExp())
  fitted.results_test= predict(ada_model,newdata=test)
  fitted.results_test = ifelse(fitted.results_test > 0.1,1,0)
  misClasificError_test = mean(fitted.results_test != test$Spam)
  ada_confmat_test = table("Y"=test$Spam,"Y hat"=fitted.results_test)
  acc[i] = sum(diag(ada_confmat_test)) / sum(ada_confmat_test)
} 


y_error <- na.omit(1-acc)
y_error = y_error[-1]
x_trees <- n
ada_m <- na.omit(data.frame(x_trees, y_error))

ggplot()+
  ggtitle(" Error for Adaboost ")+
  geom_point(data=ada_m, aes(x=x_trees, y=y_error), size=2)+
  geom_line(data=ada_m, aes(x=x_trees, y=y_error), size=1)

```

## Random Forest
The random forest algorithm is used to classify the mails as spam and non-spam.
As can be seen from the confusion matrix below (using 500 trees) the diagonal values are very high meaning that the randomForest model is predicting with high accuaracy.

```{r, echo=FALSE}
library(randomForest)
library(ggplot2)
rf<-randomForest(Spam ~ ., data= train)
  p <- predict(rf, newdata = test, type = "class")
  print(table("Y"=test$Spam,"Y hat"=p))
  misClasificError <- mean(p != test$Spam)
  misClasificError
```


Below is a plot of the recorded error for 10 up to 100 trees. 

```{r, echo=FALSE}
misClasificError <- 0
for (i in seq(10, 100, 10)) {
  rf<-randomForest(Spam ~ ., data= train, ntree=i)
  p <- predict(rf, newdata = test, type = "class")
  misClasificError[(i / 10)] <- mean(p != test$Spam)
}
rf_df<-data.frame("rferror"=misClasificError,"rftrees"=seq(10,100,10))

ggplot()+
  ggtitle(" Error for RandomForest ")+
  geom_point(data=rf_df, aes(x=rftrees, y=rferror), size=2)+
  geom_line(data=rf_df, aes(x=rftrees, y=rferror), size=1)
```

## 2. Mixture Models

## Analysis:

* After we created our data points using parameters mu and pi, the EM function takes only the data
points and guess mu and pi(pi here is like a prior probablity based on our best guess that the responsiblities are equal but it can be set differently)
* In E-step we compute the posterior responsiblities for each observation based on Bayes Theorem, if K=2 we make the assumbtion that the points are in two clusters, anf if it is 3 the we assume 3 clustersand so on.
γ(znk) = πk ∗ p(xn|µk) PK j=1 πj ∗ p(xn|µj ) , where i = 1, 2, ..., N
* In the M-step we updated our mu and pi based on the new responsiblities. µnew k = 1/Nk X N n=1 γ(znk)xn π new k = Nk N
* Log likelikelihood then is calculated (doesn’t matter if it is done before M-step or after) to check if the updated values of mu and pi are converging to the true values.
* In the case of K=2 little convergence gained after the 8th iteration and stopped at 12th iteration when we started to get minimum change. the resulted values where close to the true µ1 and µ2, this because µ3 value was 0.5 in the middle of the other clusters, so this third cluster splitted between the first two clusters without complications.
* When K=3 the convergence also start to blateau when we reached the 8th iteration and stopped at 46th iteration, this is because the values of first two true mus overlapping the the third mu, making it complicated for the algorithm to distinguish the third cluster from the other two (which are more
distinct from each other).
* The same thing happened when K=4 also the convergence also start to blateau when we reached the
8th iteration and stopped at 32nd iteration this time, this is because the third cluster around true Mu3
got split into two cluster, one that is close to the first Mu and the other close to the second Mu.
* We can conclude from fact that the algorithm start to gain little convergence at the 8th iteration is because the first two original clusters are more distinguishable from the third cluster because their Mus overlapp the third mu, and once the estimated Mus for these two clusters are gained, the algorithm find it difficult to recognize the third cluster.
```{r, echo=FALSE}
set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions
true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,],
type="o",
col="blue",
ylim=c(0,1),
main = "Original Data",
xlab = "Number of Dimensions",
ylab = "True Mu")
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
k <- sample(1:3,1,prob=true_pi)
for(d in 1:D) {
x[n,d] <- rbinom(1,1,true_mu[k,d])
}
}
EM <- function(c){
K=c # number of guessed components
z <- matrix(nrow=N, ncol=K) # fractional component assignments
pi <- vector(length = K) # mixing coefficients
mu <- matrix(nrow=K, ncol=D) # conditional distributions
llik <- vector(length = max_it) # log likelihood of the EM iterations
# Random initialization of the paramters
pi <- runif(K,0.49,0.51)
pi <- pi / sum(pi)
for(k in 1:K) {
mu[k,] <- runif(D,0.49,0.51)
}
pi
mu
for(it in 1:max_it) {
#plot(mu[1,], type="o", col="blue", ylim=c(0,1))
#points(mu[2,], type="o", col="red")
#points(mu[3,], type="o", col="green")
#points(mu[4,], type="o", col="yellow")
#Sys.sleep(0.5)
# E-step: Computation of the fractional component assignments (responsiblities)
# Your code here
for (n in 1:N) {
phi = c()
for (j in 1:K) {
y1 = mu[j,]^x[n,]
y2 = (1- mu[j,])^(1-x[n,])
phi = c(phi, prod(y1,y2))
}
z[n,] = (pi*phi) / sum(pi*phi)
}
#Log likelihood computation.
# Your code here
likelihood <-matrix(0,1000,K)
llik[it] <-0
for(n in 1:N)
{
for (k in 1:K)
{
likelihood[n,k] <- pi[k]*prod( ((mu[k,]^x[n,])*((1-mu[k,])^(1-x[n,]))))
}
llik[it]<- sum(log(rowSums(likelihood)))
}
cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
flush.console()
# Stop if the lok likelihood has not changed significantly
# Your code here
if (it > 1)
{
if (llik[it]-llik[it-1] < min_change)
{
if(K == 2)
{
plot(mu[1,],
type="o",
col="blue",
ylim=c(0,1),
main = "K = 2",
xlab = "Number of Dimensions",
ylab = "Estimated Mu")
points(mu[2,], type="o", col="red")
}
else if(K==3)
{
plot(mu[1,],
type="o",
col="blue",
ylim=c(0,1),
main = "K = 3",
xlab = "Number of Dimensions",
ylab = "Estimated Mu")
points(mu[2,], type="o", col="red")
points(mu[3,], type="o", col="green")
}
else
{
plot(mu[1,],
type="o",
col="blue",
ylim=c(0,1),
main = "K = 4",
xlab = "Number of Dimensions",
ylab = "Estimated Mu")
points(mu[2,], type="o", col="red")
points(mu[3,], type="o", col="green")
points(mu[4,], type="o", col="yellow")
}
break()
}
}
#M-step: ML parameter estimation from the data and fractional component assignments
# Your code here
mu<- (t(z) %*% x) /colSums(z)
# N - Total no. of observations
pi <- colSums(z)/N
}
pi
mu
plot(llik[1:it],
type="o",
main = "Log Likelihood",
xlab = "Number of Iterations",
ylab = "Log Likelihood")
}
EM(2)
EM(3)
EM(4)
```

##Apendix

```{r, eval=FALSE, echo=TRUE}

library(mboost)
library(readxl)
library(randomForest)
library(ggplot2)

#setwd("C:/Users/Bjorn/Documents/LIU/machine_learning")
#data=read_excel("spambase.xlsx")

sp = read.csv2("spambase_lab1_block2.csv")
sp$Spam = as.factor(sp$Spam)

n=dim(sp)[1]
set.seed(12345)
id=sample(1:n, floor(n*(2/3)))
train=sp[id,]
test=sp[-id,]

## Adaboost ##

ada_model = blackboost(Spam ~., data = train, control = boost_control(mstop = 100, nu=0.6),
                          family = AdaExp())

acc=0
n = seq(10, 100, by = 10)
for(i in n){
  ada_model = blackboost(Spam ~., data = train, control = boost_control(mstop = i, nu = 0.6),
                       family = AdaExp())
  fitted.results_test= predict(ada_model,newdata=test)
  fitted.results_test = ifelse(fitted.results_test > 0.1,1,0)
  misClasificError_test = mean(fitted.results_test != test$Spam)
  ada_confmat_test = table("Y"=test$Spam,"Y hat"=fitted.results_test)
  acc[i] = sum(diag(ada_confmat_test)) / sum(ada_confmat_test)
} 
acc

y_error <- na.omit(1-acc)
y_error = y_error[-1]
x_trees <- n
ada_m <- na.omit(data.frame(x_trees, y_error))

ggplot()+
  ggtitle(" Error for Adaboost ")+
  geom_point(data=ada_m, aes(x=x_trees, y=y_error), size=2)+
  geom_line(data=ada_m, aes(x=x_trees, y=y_error), size=1)


#train data
fitted.results_train = predict(ada_model,newdata=train)
fitted.results_train = ifelse(fitted.results_train > 0.1,1,0)
misClasificError_train = mean(fitted.results_train != train$Spam)
ada_confmat_train = table("Y"=train$Spam,"Y hat"=fitted.results_train)
ada_confmat_train
print(paste('Accuracy:',sum(diag(ada_confmat_train)) / sum(ada_confmat_train)))

#test data
fitted.results_test= predict(ada_model,newdata=test)
fitted.results_test = ifelse(fitted.results_test > 0.1,1,0)
misClasificError_test = mean(fitted.results_test != test$Spam)
ada_confmat_test = table("Y"=test$Spam,"Y hat"=fitted.results_test)
print(paste('Accuracy:',sum(diag(ada_confmat_test)) / sum(ada_confmat_test)))
#print(paste('Accuracy:',1-misClasificError_test))

## Random Forest ##

rf<-randomForest(Spam ~ ., data= train)
  p <- predict(rf, newdata = test, type = "class")
  print(table("Y"=test$Spam,"Y hat"=p))
  misClasificError <- mean(p != test$Spam)
  misClasificError
  
misClasificError <- 0
for (i in seq(10, 100, 10)) {
  rf<-randomForest(Spam ~ ., data= train, ntree=i)
  p <- predict(rf, newdata = test, type = "class")
  misClasificError[(i / 10)] <- mean(p != test$Spam)
}
rf_df<-data.frame("rferror"=misClasificError,"rftrees"=seq(10,100,10))

ggplot()+
  ggtitle(" Error for RandomForrest ")+
  geom_point(data=rf_df, aes(x=rftrees, y=rferror), size=2)+
  geom_line(data=rf_df, aes(x=rftrees, y=rferror), size=1)


## Mixed models / EM algorithm ##

set.seed(1234567890)
max_it <- 100 # max number of EM iterations
min_change <- 0.1 # min change in log likelihood between two consecutive EM iterations
N=1000 # number of training points
D=10 # number of dimensions
x <- matrix(nrow=N, ncol=D) # training data
true_pi <- vector(length = 3) # true mixing coefficients
true_mu <- matrix(nrow=3, ncol=D) # true conditional distributions

true_pi=c(1/3, 1/3, 1/3)
true_mu[1,]=c(0.5,0.6,0.4,0.7,0.3,0.8,0.2,0.9,0.1,1)
true_mu[2,]=c(0.5,0.4,0.6,0.3,0.7,0.2,0.8,0.1,0.9,0)
true_mu[3,]=c(0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5)
plot(true_mu[1,],
     type="o",
     col="blue",
     ylim=c(0,1),
     main = "Original Data",
     xlab = "Number of Dimensions",
     ylab = "True Mu")
points(true_mu[2,], type="o", col="red")
points(true_mu[3,], type="o", col="green")
# Producing the training data
for(n in 1:N) {
  k <- sample(1:3,1,prob=true_pi)
  for(d in 1:D) {
    x[n,d] <- rbinom(1,1,true_mu[k,d])
  }
}
em_algorithm <- function(c){
  K=c # number of guessed components
  z <- matrix(nrow=N, ncol=K) # fractional component assignments
  pi <- vector(length = K) # mixing coefficients
  mu <- matrix(nrow=K, ncol=D) # conditional distributions
  llik <- vector(length = max_it) # log likelihood of the EM iterations
  # Random initialization of the paramters
  pi <- runif(K,0.49,0.51)
  pi <- pi / sum(pi)
  for(k in 1:K) {
    mu[k,] <- runif(D,0.49,0.51)
  }
  pi
  mu
  for(it in 1:max_it) {
    #plot(mu[1,], type="o", col="blue", ylim=c(0,1))
    #points(mu[2,], type="o", col="red")
    #points(mu[3,], type="o", col="green")
    #points(mu[4,], type="o", col="yellow")
    #Sys.sleep(0.5)
    # E-step: Computation of the fractional component assignments (responsiblities)
    # Your code here
    for (n in 1:N){
      phi = c()
      for (j in 1:K){
        y1 = mu[j,]^x[n,]
        y2 = (1- mu[j,])^(1-x[n,])
        phi = c(phi, prod(y1,y2))
      }
      
      z[n,] = (pi*phi) / sum(pi*phi)
    }
    #Log likelihood computation.
    # Your code here
    likelihood = matrix(0,1000,K)
    llik[it] = 0
    for(n in 1:N){
      for (k in 1:K){
        likelihood[n,k] = pi[k]*prod( ((mu[k,]^x[n,])*((1-mu[k,])^(1-x[n,]))))
      }
      llik[it] = sum(log(rowSums(likelihood)))
    }
    cat("iteration: ", it, "log likelihood: ", llik[it], "\n")
    flush.console()
    # Stop if the lok likelihood has not changed significantly
    # Your code here
    if (it > 1){
      if (llik[it]-llik[it-1] < min_change){
        if(K == 2){
          plot(mu[1,],
               type="o",
               col="blue",
               ylim=c(0,1),
               main = "K = 2",
               xlab = "Number of Dimensions",
               ylab = "Estimated Mu")
          points(mu[2,], type="o", col="red")
        }
        else if(K == 3){
          plot(mu[1,],
               type="o",
               col="blue",
               ylim=c(0,1),
               main = "K = 3",
               xlab = "Number of Dimensions",
               ylab = "Estimated Mu")
          points(mu[2,], type="o", col="red")
          points(mu[3,], type="o", col="green")
        }
        else if(K == 4){
          plot(mu[1,],
               type="o",
               col="blue",
               ylim=c(0,1),
               main = "K = 4",
               xlab = "Number of Dimensions",
               ylab = "Estimated Mu")
          points(mu[2,], type="o", col="red")
          points(mu[3,], type="o", col="green")
          points(mu[4,], type="o", col="yellow")
        }
        break()
      }
    }
    #M-step: ML parameter estimation from the data and fractional component assignments
    # Your code here
    mu = (t(z) %*% x) /colSums(z)
    # N - Total no. of observations
    pi = colSums(z)/N
  }
  pi
  mu
  # plot(llik[1:it],
  #      type="o",
  #      main = "Log Likelihood",
  #      xlab = "Number of Iterations",
  #      ylab = "Log Likelihood")
}

em_algorithm(2)
em_algorithm(3)
em_algorithm(4)

```
